import requests
from bs4 import BeautifulSoup
import time
import logging
import re
from urllib.parse import urljoin, urlparse
from config import Config

class ELearningScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': Config.USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.logger = logging.getLogger(__name__)
        self.is_logged_in = False
        self.login_token = None
        
    def login(self):
        """Se connecter √† la plateforme eLearning via HTTP"""
        try:
            self.logger.info("üîê Tentative de connexion √† eLearning...")
            
            # √âtape 1: R√©cup√©rer la page de connexion pour obtenir les tokens CSRF
            login_url = f"{Config.ELEARNING_URL}/login/index.php"
            response = self.session.get(login_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extraire le token de connexion (logintoken)
            login_token_input = soup.find('input', {'name': 'logintoken'})
            if login_token_input:
                self.login_token = login_token_input.get('value')
                self.logger.info(f"‚úÖ Token de connexion r√©cup√©r√©: {self.login_token[:10]}...")
            else:
                self.logger.warning("‚ö†Ô∏è Aucun token de connexion trouv√©")
            
            # √âtape 2: Effectuer la connexion
            login_data = {
                'username': Config.USERNAME,
                'password': Config.PASSWORD,
                'logintoken': self.login_token or '',
            }
            
            # Headers pour la requ√™te POST
            headers = {
                'Content-Type': 'application/x-www-form-urlencoded',
                'Referer': login_url,
            }
            
            response = self.session.post(login_url, data=login_data, headers=headers, timeout=30)
            response.raise_for_status()
            
            # V√©rifier si la connexion a r√©ussi
            if self._check_login_success(response):
                self.is_logged_in = True
                self.logger.info("‚úÖ Connexion r√©ussie √† eLearning!")
                return True
            else:
                self.logger.error("‚ùå √âchec de la connexion - identifiants incorrects ou probl√®me de session")
                return False
                
        except requests.exceptions.RequestException as e:
            self.logger.error(f"‚ùå Erreur r√©seau lors de la connexion: {str(e)}")
            return False
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de la connexion: {str(e)}")
            return False
    
    def _check_login_success(self, response):
        """V√©rifier si la connexion a r√©ussi"""
        try:
            # V√©rifier la redirection vers le dashboard
            if response.url and 'login' not in response.url:
                self.logger.info(f"üîÑ Redirection d√©tect√©e vers: {response.url}")
                return True
            
            # V√©rifier le contenu de la page pour des indicateurs de succ√®s
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Chercher des √©l√©ments qui indiquent une connexion r√©ussie
            success_indicators = [
                soup.find('div', {'class': 'user-menu'}),
                soup.find('div', {'class': 'usermenu'}),
                soup.find('a', {'href': lambda x: x and 'logout' in x}),
                soup.find('div', {'class': 'dashboard'}),
                soup.find('div', {'class': 'course-content'}),
            ]
            
            if any(success_indicators):
                self.logger.info("‚úÖ Indicateurs de connexion r√©ussie d√©tect√©s")
                return True
            
            # V√©rifier s'il y a des messages d'erreur
            error_indicators = [
                soup.find('div', {'class': 'alert-danger'}),
                soup.find('div', {'class': 'error'}),
                soup.find(text=re.compile(r'Invalid login|incorrect|failed', re.I)),
            ]
            
            if any(error_indicators):
                self.logger.error("‚ùå Messages d'erreur de connexion d√©tect√©s")
                return False
            
            # Si on arrive ici, c'est ambigu - on consid√®re comme un √©chec par s√©curit√©
            self.logger.warning("‚ö†Ô∏è Statut de connexion ambigu")
            return False
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de la v√©rification de connexion: {str(e)}")
            return False
    
    def get_course_content(self, course_url, course_id):
        """R√©cup√©rer le contenu d'un cours sp√©cifique via HTTP"""
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                # V√©rifier la connexion
                if not self.is_logged_in:
                    if not self.login():
                        return None
                
                self.logger.info(f"üìñ R√©cup√©ration du contenu pour le cours {course_id}")
                
                # R√©cup√©rer la page du cours
                response = self.session.get(course_url, timeout=30)
                response.raise_for_status()
                
                # V√©rifier si on est toujours connect√©
                if 'login' in response.url:
                    self.logger.warning("‚ö†Ô∏è Session expir√©e, reconnexion...")
                    self.is_logged_in = False
                    if not self.login():
                        return None
                    # R√©essayer la requ√™te
                    response = self.session.get(course_url, timeout=30)
                    response.raise_for_status()
                
                # Parser le contenu
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Extraire le contenu principal
                content = {
                    'course_id': course_id,
                    'url': course_url,
                    'timestamp': time.time(),
                    'sections': [],
                    'title': self._extract_course_title(soup),
                    'raw_content': str(soup)  # Garder le contenu brut pour analyse
                }
                
                # R√©cup√©rer les sections du cours
                sections = self._extract_sections(soup)
                content['sections'] = sections
                
                self.logger.info(f"‚úÖ Contenu r√©cup√©r√© pour le cours {course_id}: {len(sections)} sections")
                return content
                
            except requests.exceptions.RequestException as e:
                retry_count += 1
                self.logger.warning(f"‚ö†Ô∏è Tentative {retry_count}/{max_retries} √©chou√©e pour le cours {course_id}: {str(e)}")
                
                if retry_count < max_retries:
                    time.sleep(2)  # Attendre avant de r√©essayer
                else:
                    self.logger.error(f"‚ùå √âchec d√©finitif pour le cours {course_id} apr√®s {max_retries} tentatives")
                    return None
            except Exception as e:
                retry_count += 1
                self.logger.error(f"‚ùå Erreur inattendue pour le cours {course_id}: {str(e)}")
                
                if retry_count < max_retries:
                    time.sleep(2)
                else:
                    return None
        
        return None
    
    def _extract_course_title(self, soup):
        """Extraire le titre du cours"""
        try:
            # Essayer diff√©rents s√©lecteurs pour le titre
            title_selectors = [
                'h1.course-title',
                'h1',
                '.page-header h1',
                '.course-header h1',
                'title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text().strip()
                    if title and len(title) > 3:  # √âviter les titres trop courts
                        return title
            
            return "Titre non trouv√©"
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erreur lors de l'extraction du titre: {str(e)}")
            return "Titre non trouv√©"
    
    def _extract_sections(self, soup):
        """Extraire les sections du cours"""
        sections = []
        
        try:
            # Essayer diff√©rents s√©lecteurs pour les sections
            section_selectors = [
                '.section',
                '.course-section',
                '.course-content .section',
                '.course-content > div',
                '.course-content .activity',
            ]
            
            section_elements = []
            for selector in section_selectors:
                elements = soup.select(selector)
                if elements:
                    section_elements = elements
                    self.logger.info(f"üìã Trouv√© {len(elements)} sections avec le s√©lecteur: {selector}")
                    break
            
            if not section_elements:
                # Si aucune section sp√©cifique trouv√©e, essayer de r√©cup√©rer tout le contenu
                self.logger.warning("‚ö†Ô∏è Aucune section sp√©cifique trouv√©e, extraction du contenu g√©n√©ral")
                main_content = soup.select_one('.course-content') or soup.select_one('main') or soup.select_one('body')
                if main_content:
                    section_elements = [main_content]
            
            for section in section_elements:
                section_data = self._extract_section_data(section)
                if section_data:
                    sections.append(section_data)
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de l'extraction des sections: {str(e)}")
        
        return sections
    
    def _extract_section_data(self, section_element):
        """Extraire les donn√©es d'une section"""
        try:
            section_data = {
                'title': '',
                'activities': [],
                'resources': [],
                'content': '',
                'links': []
            }
            
            # Titre de la section
            title_selectors = ['.sectionname', '.section-title', 'h2', 'h3', '.title']
            for selector in title_selectors:
                title_elem = section_element.select_one(selector)
                if title_elem:
                    section_data['title'] = title_elem.get_text().strip()
                    break
            
            # Si pas de titre sp√©cifique, utiliser le texte principal
            if not section_data['title']:
                section_data['title'] = section_element.get_text().strip()[:100] + "..." if len(section_element.get_text().strip()) > 100 else section_element.get_text().strip()
            
            # Contenu textuel
            section_data['content'] = section_element.get_text().strip()
            
            # Liens et activit√©s
            links = section_element.find_all('a', href=True)
            for link in links:
                link_data = {
                    'text': link.get_text().strip(),
                    'url': urljoin(Config.ELEARNING_URL, link['href']),
                    'title': link.get('title', '')
                }
                section_data['links'].append(link_data)
                
                # Classifier comme activit√© ou ressource
                if any(keyword in link_data['text'].lower() for keyword in ['forum', 'discussion', 'chat']):
                    section_data['activities'].append({
                        'title': link_data['text'],
                        'type': 'forum',
                        'url': link_data['url']
                    })
                elif any(keyword in link_data['text'].lower() for keyword in ['devoir', 'assignment', 'travail']):
                    section_data['activities'].append({
                        'title': link_data['text'],
                        'type': 'assignment',
                        'url': link_data['url']
                    })
                elif any(keyword in link_data['text'].lower() for keyword in ['fichier', 'document', 'pdf', 'doc']):
                    section_data['resources'].append({
                        'title': link_data['text'],
                        'type': 'file',
                        'url': link_data['url']
                    })
                else:
                    section_data['resources'].append({
                        'title': link_data['text'],
                        'type': 'link',
                        'url': link_data['url']
                    })
            
            return section_data
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de l'extraction des donn√©es de section: {str(e)}")
            return None
    
    def get_all_courses_content(self):
        """R√©cup√©rer le contenu de tous les cours surveill√©s"""
        all_content = {}
        successful_scans = 0
        failed_scans = 0
        
        self.logger.info(f"üîç D√©but du scan de {len(Config.MONITORED_SPACES)} espaces d'affichage")
        
        # S'assurer qu'on est connect√©
        if not self.is_logged_in:
            if not self.login():
                self.logger.error("‚ùå Impossible de se connecter, arr√™t du scan")
                return {}
        
        for i, space in enumerate(Config.MONITORED_SPACES, 1):
            self.logger.info(f"[{i}/{len(Config.MONITORED_SPACES)}] R√©cup√©ration du contenu pour: {space['name']}")
            
            try:
                content = self.get_course_content(space['url'], space['id'])
                if content:
                    all_content[space['id']] = content
                    successful_scans += 1
                    self.logger.info(f"‚úÖ Succ√®s pour: {space['name']}")
                else:
                    failed_scans += 1
                    self.logger.error(f"‚ùå √âchec pour: {space['name']}")
            except Exception as e:
                failed_scans += 1
                self.logger.error(f"‚ùå Erreur pour {space['name']}: {str(e)}")
            
            # Pause entre les requ√™tes pour √©viter la surcharge
            time.sleep(2)
        
        self.logger.info(f"üìä Scan termin√©: {successful_scans} succ√®s, {failed_scans} √©checs")
        return all_content
    
    def close(self):
        """Fermer la session"""
        if self.session:
            self.session.close()
            self.logger.info("üîí Session ferm√©e")